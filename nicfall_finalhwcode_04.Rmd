---
title: "nicfall_finalhwcode_04"
author: "Nicola Kriefall"
date: "10/24/2019"
output: html_document
---

HW 4, instructions [here](https://fuzzyatelin.github.io/bioanth-stats/homework-04.html)

[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:

Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().

The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.

The function should contain a check for the rules of thumb we have talked about (n∗p>5 and n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.

The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

```{r}
Z.prop.test <- function(p0, p1, n1, p2 = NULL, n2 = NULL, alternative="two.sided", conf.level=0.95){
  if(is.null(p2) && is.null(n2)){
    if(n1*p0 <= 5){print("Warning: assumption of normality (n1*p0 > 5) violated")}
    if(n1*(1-p0) <= 5){print("Warning: assumption of normality (n1*(1-p0)) violated")}
    if(n1*p1 <= 5){print("Warning: assumption of normality (n1*p1 > 5) violated")}
    if(n1*(1-p1) <= 5){print("Warning: assumption of normality (n1*(1-p1)) violated")}
    if(alternative=="two.sided"){
     m1 <- p1 # mean, observed
     z1 <- (m1 - p0)/sqrt(p0 * (1 - p0)/n1)
     pv1 <- 1 - pnorm(z1,lower.tail=TRUE) + pnorm(z1,lower.tail=FALSE)
     perc <- (1-((1-conf.level)/2))
     lower <- p1 - qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     upper <- p1 + qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     ci <- c(lower, upper)
     print(c("z-statistic:",z1))
     print(c("p-value:",pv1))
     print(c("ci (lower,upper):",ci))
    }
    if(alternative=="less"){
     m1 <- p1 # mean, observed
     z1 <- (m1 - p0)/sqrt(p0 * (1 - p0)/n1)
     pv1 <- pnorm(z1, lower.tail = TRUE)
     perc <- (1-((1-conf.level)/2))
     lower <- p1 - qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     upper <- p1 + qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     ci <- c(lower, upper)
     print(c("z-statistic:",z1))
     print(c("p-value:",pv1))
     print(c("ci (lower,upper):",ci))
    }
    if(alternative=="greater"){
     m1 <- p1 # mean, observed
     z1 <- (m1 - p0)/sqrt(p0 * (1 - p0)/n1)
     pv1 <- pnorm(z1, lower.tail = FALSE)
     perc <- (1-((1-conf.level)/2))
     lower <- p1 - qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     upper <- p1 + qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     ci <- c(lower, upper)
     print(c("z-statistic:",z1))
     print(c("p-value:",pv1))
     print(c("ci (lower,upper):",ci))
    }
    }
  else{
    #am I supposed to do the p0 calculation for rule of thumb here too? with which n?
    if(n1*p1 <= 5){print("Warning: assumption of normality (n1*p1 > 5) violated")}
    if(n1*(1-p1) <= 5){print("Warning: assumption of normality (n1*(1-p1)) violated")}
    if(n2*p2 <= 5){print("Warning: assumption of normality (n2*p2 > 5) violated")}
    if(n2*(1-p2) <= 5){print("Warning: assumption of normality (n2*(1-p2)) violated")}
    if(alternative=="two.sided"){
     num1 <- p1*n1
     num2 <- p2*n2
     pstar <- (num1 + num2)/(n1+n2)
     z1 <- (p2 - p1 - p0)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
     pv1 <- 1 - pnorm(z1,lower.tail=TRUE) + pnorm(z1,lower.tail=FALSE)
     perc <- (1-((1-conf.level)/2))
     lower <- p1 - qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     upper <- p1 + qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     ci <- c(lower, upper)
     print(c("z-statistic:",z1))
     print(c("p-value:",pv1))
     print(c("ci (lower,upper):",ci))
    }
    if(alternative=="less"){
     num1 <- p1*n1
     num2 <- p2*n2
     pstar <- (num1 + num2)/(n1+n2)
     z1 <- (p2 - p1 - p0)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
     pv1 <- pnorm(z1,lower.tail=TRUE)
     perc <- (1-((1-conf.level)/2))
     lower <- p1 - qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     upper <- p1 + qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     ci <- c(lower, upper)
     print(c("z-statistic:",z1))
     print(c("p-value:",pv1))
     print(c("ci (lower,upper):",ci))
    }
    if(alternative=="greater"){
     num1 <- p1*n1
     num2 <- p2*n2
     pstar <- (num1 + num2)/(n1+n2)
     z1 <- (p2 - p1 - p0)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
     pv1 <- pnorm(z1,lower.tail=FALSE)
     perc <- (1-((1-conf.level)/2))
     lower <- p1 - qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     upper <- p1 + qnorm(perc) * sqrt(p1 * (1 - p1)/n1)
     ci <- c(lower, upper)
     print(c("z-statistic:",z1))
     print(c("p-value:",pv1))
     print(c("ci (lower,upper):",ci))
    }
    } #closes the else for having p2 & n2
  } #final closing mark
```

[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

```{r}
#first, read in data & necessary packages:
library(curl)
library(ggplot2)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)

#primitive looks at the data using base plot
lm(data=d,MaxLongevity_m~Brain_Size_Species_Mean)
#plot(data=d,MaxLongevity_m~Brain_Size_Species_Mean)

lm(data=d,log(MaxLongevity_m)~log(Brain_Size_Species_Mean))
#plot(data=d,log(MaxLongevity_m)~log(Brain_Size_Species_Mean))

#now a ggplot look at the data
ggplot(data=d,aes(x=Brain_Size_Species_Mean,y=MaxLongevity_m))+
  geom_point()+
  theme_classic()+
  geom_abline(intercept=248.952, slope=1.218)+
  geom_text(x=50,y=725,label="y = 1.218x + 248.952")

ggplot(data=d,aes(x=log(Brain_Size_Species_Mean),y=log(MaxLongevity_m)))+
  geom_point()+
  theme_classic()+
  geom_abline(intercept=4.879, slope=0.2341)+
  geom_text(x=1,y=6.5,label="y = 0.2341x + 4.879")
```

Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

```{r}
lm1 <- lm(data=d,MaxLongevity_m~Brain_Size_Species_Mean)
summary(lm1)
#point estimate of the slope (β1) = 1.218
#if β1 = 0, then you get the intercept 248.952
#I think the output here is saying that we reject the null hypothesis, since the regression is super significant, i.e. β1 ≠ 0

lmlog <- lm(data=d,log(MaxLongevity_m)~log(Brain_Size_Species_Mean))
summary(lmlog)
#point estimate of the slope (β1) = 0.23415
#if β1 = 0, then you get the intercept 4.87895
#I think the output here is saying that we reject the null hypothesis, since the regression is super significant, i.e. β1 ≠ 0

#finding 90% confidence intervals for untransformed & log transformed data:
ci <- predict(lm1, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "confidence", level = 0.90)  # for a vector of values
df <- data.frame(cbind(d$Brain_Size_Species_Mean, d$MaxLongevity_m))
df <- cbind(df, ci)
names(df) <- c("x", "y", "CIfit", "CIlwr", "CIupr")

d$logbrain <- log(d$Brain_Size_Species_Mean) #making this a bit simpler for myself
d$loglife <- log(d$MaxLongevity_m)
lmlog <- lm(data=d,loglife~logbrain)
ci.log <- predict(lmlog, newdata = data.frame(logbrain = d$logbrain), interval = "confidence", level = 0.90)  # for a vector of values
df.log <- data.frame(cbind(d$logbrain, d$loglife))
df.log <- cbind(df.log, ci.log)
names(df.log) <- c("x", "y", "CIfit", "CIlwr", "CIupr")
```

Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

```{r}
ggplot(data=df,aes(x=x,y=y))+
  geom_point(alpha=0.5)+
  xlab("Brain size (g; species mean)")+
  ylab("Max longevity (months)")+
  theme_classic()+
  geom_line(aes(x = x, y = CIupr, color = "90% CI"))+
  geom_line(aes(x = x, y = CIfit, color = "Best fit"))+
  geom_line(aes(x = x, y = CIlwr, color = "90% CI"))+
  geom_text(x=75,y=850,label="y = 1.218x + 248.952")+
  scale_color_manual(values=c("90% CI" = "blueviolet", "Best fit" = "black"))+
  labs(color="")

ggplot(data=df.log,aes(x=x,y=y))+
  geom_point(alpha=0.5)+
  xlab("Log of brain size (g; species mean)")+
  ylab("Log of max longevity (months)")+
  theme_classic()+
  geom_line(aes(x = x, y = CIupr, color = "90% CI"))+
  geom_line(aes(x = x, y = CIfit, color = "Best fit"))+
  geom_line(aes(x = x, y = CIlwr, color = "90% CI"))+
  geom_text(x=1.25,y=6.5,label="y = 0.2341x + 4.879")+
  scale_color_manual(values=c("90% CI" = "blueviolet", "Best fit" = "black"))+
  labs(color="")
```

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

```{r}
#from my model equation above: (y = 1.218x + 248.952)
h_hat <- 1.218 * 800 + 248.952
h_hat #for a species whose brain weight is 800 g, the predicted longevity is 1223.345 months

#another way of doing it, same answer + confidence intervals
pred <- predict(lm1, newdata=data.frame(Brain_Size_Species_Mean=800), interval="confidence",leve=0.90)
pred #answers to Chris' questions! I somewhat trust this result, but not fully because the data used to make the model was not normally distributed (regression plot doesn't really look linear & I checked the 4 plots from lm1 output)

#I don't think I can do this as easily for the log transformed model because it's the log, but here goes:
log(800) #plugging this into the formula instead of 800 g regular
h_hat.log <- 0.2341 * 6.684612 + 4.879 #from my model equation above: y = 0.2341x + 4.879
h_hat.log #for a species whose brain weight is 800 g, the predicted longevity is 1223.345 months
exp(6.44415) #'unlogging', result for longevity is 629.0118 months, very different than other answer.. 

#another way of doing it, same answer + confidence intervals
pred.log <- predict(lmlog, newdata=data.frame(logbrain=6.684612), interval="confidence",leve=0.90)
pred.log #answers to Chris' questions! I don't trust it as much since Chris is always saying interpreting log-transformed data is tricky
```

Q: Looking at your two models, which do you think is better? Why?

A: Considering output below, I think the log-transformed data better satisfies the normal distribution assumptions of the linear model, but this makes interpreting the result of asking what the longevity of an 800 g brain species would be more difficult.. 

```{r}
par(mfrow=c(2,2))
plot(lm1) #could be better
par(mfrow=c(2,2))
plot(lmlog) #better
```

# Challenges

1. Couldn't get through this hw the first time I tried it :( (the function half, not the linear model half)

2. I still don't fully understand the p x n > 5 rule of thumb, is this for every proportion that you have or just pi (p0; the expected population proportion). I did n x p for all the combinations (p0 x n1, p1 x n1, p2 x n2) just to be safe. I'm confused about using the same 'n1' for both p0 & p1 - is that the right thing to do or no? Also if you have a p0 with 2 samples (p1 & p2), would you do p0 x n1 or n2 or both or neither?

3. I'm sure there's a prettier way to print the function output than the way I did it haha. 

4. I always get confused about which term should be the dependent variable in the linear model - can we specify which one will be x & which one will be y? I *think* you get slightly different answers depending on which one goes first in the formula

5. I wasn't sure how to do this part: "the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0." but I tried

6. Figuring out a longevity prediction for a log transformed model